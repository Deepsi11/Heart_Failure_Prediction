# -*- coding: utf-8 -*-
"""Project: Heart Failure Prediction using Machine Learning .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1r-GtpviM3uo1J__pvgqtaU9NB-LLVL2S

#**IMPORTING THE DEPENDENCIES**
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import svm

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report

"""#**DATA COLLECTION**


"""

# Loading the heart stroke dataset to the pandas dataframe
heart_failure_dataset = pd.read_csv("/content/heart_failure_dataset.csv")

"""#**DATA PRE-PROCESSING**"""

# Printing the first five rows of the dataset
heart_failure_dataset.head()

# Counting the number of rows and columns of the dataset
heart_failure_dataset.shape

# Gettting the statistical measures of the dataset
heart_failure_dataset.describe()

# Counting the number of values of death events and non-deadth-events
heart_failure_dataset["DEATH_EVENT"].value_counts()

"""Label 0---> Death Does Not Occurred


Label 1---> Death Occurred
"""

# Calculating the mean of all the attributes for "Death_Event" column
heart_failure_dataset.groupby("DEATH_EVENT").mean()

# Separating the data and labels from the dataset
X = heart_failure_dataset.drop(columns="DEATH_EVENT", axis=1)
Y = heart_failure_dataset["DEATH_EVENT"]

print(X)

print(Y)

"""#**DATA STANDARDIZATION**"""

# Standardizing the dataset to make the data in same range
scaler = StandardScaler()

# Fitting the dataset to the standardized function
scaler.fit(X)

# Storing the transformed data to new variable called standardized_data
standardized_data = scaler.transform(X)

print(standardized_data)

X = standardized_data
Y = heart_failure_dataset["DEATH_EVENT"]

print(X)

print(Y)

"""#**DATA VISUALIZATION**

**STRIP PLOT TO HAVE A LOOK AT COMPLETE DATASET**
"""

# Visualizing the data through strip plot
plt.figure(figsize=(25, 4))
sns.stripplot(data=heart_failure_dataset)

"""**BAR PLOTS TO VISUALIZE CATEGORICAL VARIABLES OF THE DATASET**"""

# Selecting the variables to plot against 'Death Event'
categorical_features = ['anaemia', 'diabetes', 'high_blood_pressure', 'smoking']

# Create a grid of subplots
fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 10))
axes = axes.flatten()
sns.set_palette("Set2")


# Loop through the selected variables and create individual plots
for i, variable in enumerate(categorical_features):
    sns.barplot(x='DEATH_EVENT', y=variable, data=heart_failure_dataset, ax=axes[i])
    axes[i].set_title(f'{variable} vs DEATH_EVENT')

# Adjust layout to prevent overlapping titles
plt.tight_layout()

# Show the plots
plt.show()

"""**VIOLIN PLOT FOR VISUALIZING CONTINIOUS VARIABLES IN THE DATASET**"""

# Selecting the continious variables to plot against 'Death Event'
continuous_features = ['age', 'creatinine_phosphokinase', 'ejection_fraction', 'platelets',
                       'serum_creatinine', 'serum_sodium', 'time']

# Create a grid of subplots
fig, axes = plt.subplots(nrows=4, ncols=2, figsize=(15, 20))
axes = axes.flatten()
sns.set_palette("husl", 8)


# Loop through the selected variables and create individual plots
for i, variable in enumerate(continuous_features):
    sns.violinplot(x='DEATH_EVENT', y=variable, data=heart_failure_dataset, ax=axes[i])
    axes[i].set_title(f'{variable} vs DEATH_EVENT')

# Adjust layout to prevent overlapping titles
plt.tight_layout()

# Show the plots
plt.show()

"""#**TRAIN-TEST-SPLIT**"""

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 42 )

print(X.shape, X_train.shape, X_test.shape)

"""#**TRAINING THE MODEL**

#**SUPPORT VECTOR MACHINE**
"""

classifier = svm.SVC(kernel="linear")

# Training the Support vector machine classifier
classifier.fit(X_train, Y_train)

# Training the support vector Machine Classifier
classifier.fit(X_train, Y_train)

"""#**MODEL EVALUATION**


"""

# ACCURACY SCORE
# Accuracy score on the training data
X_train_prediction = classifier.predict(X_train)
training_data_accuracy = accuracy_score(X_train_prediction, Y_train)

print("Accuracy score of the training data : ", training_data_accuracy)

# ACCURACY SCORE
# Accuracy score on the test data
X_test_prediction = classifier.predict(X_test)
test_data_accuracy = accuracy_score(X_test_prediction, Y_test)

print("Accuracy score of the test data : ", test_data_accuracy)

# Initializing the Decision Tree classifier
decision_tree = DecisionTreeClassifier(random_state=42)

# Train the model on the training data
decision_tree.fit(X_train, Y_train)

# Make predictions on the test data
y_pred = decision_tree.predict(X_test)

# Evaluate the model's performance
accuracy = accuracy_score(Y_test, y_pred)
report = classification_report(Y_test, y_pred)

print("Accuracy:", accuracy)
print("Classification Report:\n", report)



"""#**Making a Predictive System**"""

input_data = (42,0,102,1,40,0,237000,1.2,140,1,0,74)

# Changing the input data to numpy array
input_data_as_numpy_array = np.asarray(input_data)

# Reshape the array as we are predicting for one instance
input_data_reshaped = input_data_as_numpy_array.reshape(1, -1)

# Standardize the input data
std_data = scaler.transform(input_data_reshaped)
print(std_data)

prediction = classifier.predict(std_data)
print(prediction)

if (prediction[0] == 0):
  print("The Person is still alive")
else:
  print("The Person has lost his life due to Heart Failure")